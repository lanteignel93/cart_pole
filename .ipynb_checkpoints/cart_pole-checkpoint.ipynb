{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import deque,namedtuple\n",
    "import os\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from agent import *\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agent_rewards(agent):\n",
    "    fig, ax = plt.subplots(1,figsize=(10,8))\n",
    "    ax.plot(agent.rewards)\n",
    "    ax.set_title(\"Agent Reward Progression across Episodes\", fontsize = 16)\n",
    "    ax.set_ylabel('Reward', fontsize = 14)\n",
    "    ax.set_xlabel('Episodes', fontsize = 14)\n",
    "    plt.show()\n",
    "    training_time = agent.get_train_time()\n",
    "    print('Training Time: {:.2f}s'.format(training_time))\n",
    "    print(\"Number of episodes: \", agent.n_episodes)\n",
    "    print(\"Batch Size: \", agent.batch_size)\n",
    "    print(\"Discount Factor: \", agent.gamma)\n",
    "    print(\"Learning Rate: \", agent.lr)\n",
    "    print(\"Epsilong Decay: \", agent.decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "s = env.reset()\n",
    "state_sz = env.observation_space.shape[0]\n",
    "action_sz = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_network = DQN(256,state_sz, action_sz)\n",
    "target_network = DQN(256,state_sz, action_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=500\n",
    "batch_size=128\n",
    "gamma = 0.98\n",
    "lr = 0.0001\n",
    "eps = 1.0\n",
    "decay = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = Agent(env, dq_network, target_network)\n",
    "agent1.init_hyperparameters(n_episodes, batch_size, gamma, lr, decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  4\n",
      "Action space:  2\n"
     ]
    }
   ],
   "source": [
    "agent1.print_env_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\laure\\OneDrive\\projects\\reinforcement_learning\\cart_pole\\agent.py:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  s = torch.FloatTensor([t.s for t in batch])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  100\n",
      "Transition Count:  312\n",
      "Episode Reward:  312.0\n",
      "Episode:  200\n",
      "Transition Count:  238\n",
      "Episode Reward:  238.0\n"
     ]
    }
   ],
   "source": [
    "agent1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_rewards(agent1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that potentially the agent learns how to achieve top reward of 500 then sometimes drop in between and at some point even repetitively misses the target might be due to high level of gamma which makes value updating a bit too slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "n_episodes=500\n",
    "batch_size=256\n",
    "gamma = 0.95\n",
    "lr = 0.001\n",
    "decay = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = Agent(env, dq_network, target_network, n_episodes, batch_size, gamma, lr, decay, action_sz, state_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_rewards(agent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our agent does some progress but doesn't reach the state with maximum reward any! Perhaps the discount factor here is too high or the learning rate is too high and the agent cannot properly find the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "n_episodes=500\n",
    "batch_size=128\n",
    "gamma = 0.99\n",
    "lr = 0.01\n",
    "decay = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent3 = Agent(env, dq_network, target_network, n_episodes, batch_size, gamma, lr, decay, action_sz, state_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_agent_rewards(agent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the learning rate is much higher and the agent finds the optimal solution quickly but because the learning rate is so high, it is possible through Adam optimizer to leave a good local minimum to a worse one. While it seems great to have found the solution quickly, it's not very robust and the agent will keep missing some episodes. Ideally we'd want to have a similar discount factor but lower learning rate with perhaps more episodes to make sure the agent learns well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
